{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Idriss-jedid/Automated-Contract-Photo-Verification/blob/master/Computer%20Vision%20/%20Car_Condition_Classification_Complet_vs_Noncomplet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS7EBqokRSUo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yymHiYU-W0Dy",
        "outputId": "c2971104-28a9-4297-ce1e-57fa86d3c780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhdlOL9ZWcVU"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/data_car_direction.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C37t-NM1SccN"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM7-A7x5SdcN"
      },
      "outputs": [],
      "source": [
        "class_names = df['class'].unique()\n",
        "label_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "df['class'] = df['class'].map(label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNpjq1QtSjK6"
      },
      "outputs": [],
      "source": [
        "# Define image directory\n",
        "image_dir = '/content/drive/MyDrive/filtered_data'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I0lx8wOSmJe"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.dataframe.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VH7RssbStLQ"
      },
      "outputs": [],
      "source": [
        "# Split the data into train, validation, and test sets  [0.6 0.2 0.2]\n",
        "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df['class'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPQXHEe5S1ak"
      },
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJahmRAkS4fB"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = ImageDataset(train_df, image_dir, transform=transform)\n",
        "val_dataset = ImageDataset(val_df, image_dir, transform=transform)\n",
        "test_dataset = ImageDataset(test_df, image_dir, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e348Y2_AS7cn"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM6svn30S-k6",
        "outputId": "fe23ba66-8b7a-4803-92fc-c420c22df9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 121MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFnMY9tbUWQe"
      },
      "outputs": [],
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmePewOnUZx1"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE9EBqFFUc6j"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqlYAIZgVo9z"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, criterion, data_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX1CxMHjVtCt",
        "outputId": "2b803158-eed1-42cf-bc4a-1dad03458c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19, Train Loss: 0.4468, Train Accuracy: 0.8533\n",
            "Val Loss: 0.4147, Val Accuracy: 0.8447, F1 Score: 0.8510\n",
            "Epoch 1/19, Train Loss: 0.2066, Train Accuracy: 0.9319\n",
            "Val Loss: 0.2818, Val Accuracy: 0.9036, F1 Score: 0.9006\n",
            "Epoch 2/19, Train Loss: 0.1528, Train Accuracy: 0.9440\n",
            "Val Loss: 0.3898, Val Accuracy: 0.8787, F1 Score: 0.8840\n",
            "Epoch 3/19, Train Loss: 0.1572, Train Accuracy: 0.9437\n",
            "Val Loss: 0.3475, Val Accuracy: 0.8832, F1 Score: 0.8817\n",
            "Epoch 4/19, Train Loss: 0.0775, Train Accuracy: 0.9705\n",
            "Val Loss: 0.3060, Val Accuracy: 0.9138, F1 Score: 0.9121\n",
            "Epoch 5/19, Train Loss: 0.0839, Train Accuracy: 0.9716\n",
            "Val Loss: 0.5680, Val Accuracy: 0.8277, F1 Score: 0.8376\n",
            "Epoch 6/19, Train Loss: 0.0595, Train Accuracy: 0.9819\n",
            "Val Loss: 0.3462, Val Accuracy: 0.9116, F1 Score: 0.9108\n",
            "Epoch 7/19, Train Loss: 0.0527, Train Accuracy: 0.9830\n",
            "Val Loss: 0.4056, Val Accuracy: 0.9093, F1 Score: 0.9075\n",
            "Epoch 8/19, Train Loss: 0.0379, Train Accuracy: 0.9887\n",
            "Val Loss: 0.2961, Val Accuracy: 0.9184, F1 Score: 0.9186\n",
            "Epoch 9/19, Train Loss: 0.0519, Train Accuracy: 0.9837\n",
            "Val Loss: 0.3570, Val Accuracy: 0.9150, F1 Score: 0.9129\n",
            "Epoch 10/19, Train Loss: 0.0461, Train Accuracy: 0.9830\n",
            "Val Loss: 0.2726, Val Accuracy: 0.9297, F1 Score: 0.9300\n",
            "Epoch 11/19, Train Loss: 0.0239, Train Accuracy: 0.9924\n",
            "Val Loss: 0.3182, Val Accuracy: 0.9206, F1 Score: 0.9190\n",
            "Epoch 12/19, Train Loss: 0.0309, Train Accuracy: 0.9887\n",
            "Val Loss: 0.3744, Val Accuracy: 0.9104, F1 Score: 0.9131\n",
            "Epoch 13/19, Train Loss: 0.0748, Train Accuracy: 0.9750\n",
            "Val Loss: 0.6234, Val Accuracy: 0.8571, F1 Score: 0.8638\n",
            "Epoch 14/19, Train Loss: 0.0685, Train Accuracy: 0.9762\n",
            "Val Loss: 0.3671, Val Accuracy: 0.9070, F1 Score: 0.9094\n",
            "Epoch 15/19, Train Loss: 0.0306, Train Accuracy: 0.9890\n",
            "Val Loss: 0.4036, Val Accuracy: 0.9150, F1 Score: 0.9147\n",
            "Epoch 16/19, Train Loss: 0.0370, Train Accuracy: 0.9871\n",
            "Val Loss: 0.4368, Val Accuracy: 0.9116, F1 Score: 0.9093\n",
            "Epoch 17/19, Train Loss: 0.0161, Train Accuracy: 0.9943\n",
            "Val Loss: 0.2898, Val Accuracy: 0.9399, F1 Score: 0.9396\n",
            "Epoch 18/19, Train Loss: 0.0108, Train Accuracy: 0.9966\n",
            "Val Loss: 0.3538, Val Accuracy: 0.9320, F1 Score: 0.9322\n",
            "Epoch 19/19, Train Loss: 0.0087, Train Accuracy: 0.9958\n",
            "Val Loss: 0.5117, Val Accuracy: 0.9172, F1 Score: 0.9143\n"
          ]
        }
      ],
      "source": [
        "num_epochs=20\n",
        "best_model_wts = None\n",
        "best_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, criterion, optimizer, train_loader, device)\n",
        "    val_loss, val_acc, val_f1 = evaluate_model(model, criterion, val_loader, device)\n",
        "\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}')\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_model_wts = model.state_dict()\n",
        "        torch.save(model.state_dict(), 'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgjvlB35s5Rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8994086e-74ea-4dc1-d959-e90377960be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-97ca0473330f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMP-JYR_WEMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e14c762-45af-4506-9a91-303ab695d0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Weighted F1 Score: 0.9338\n",
            "Test Accuracy: 0.9342\n",
            "Test Loss: 0.2581\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "total_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # Make sure to import nn from torch\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "accuracy = correct / total\n",
        "avg_loss = total_loss / total\n",
        "\n",
        "print(f'Test Weighted F1 Score: {f1:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4MAuVq2DAh6",
        "outputId": "0ce08137-41b1-47e5-f13c-4b958bfa6af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import onnx\n",
        "\n",
        "# Your class names\n",
        "class_names = ['autre', 'avant', 'arrier', 'droite', 'gauche']\n",
        "\n",
        "# Load pre-trained ResNet-18 model and modify the final layer\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))  # Adjusting the output layer to match the number of classes\n",
        "\n",
        "# Load the state dictionary from your trained model\n",
        "state_dict = torch.load(\"best_model.pth\", map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export the model to ONNX format\n",
        "torch.onnx.export(model, dummy_input, \"modelresnet.onnx\", input_names=['input'], output_names=['class_scores'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AEvWTRt0K5z",
        "outputId": "4905854c-a732-4a8b-df3a-d1faf0b5f3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-22-18bd9951839e>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"best_model.pth\", map_location=torch.device('cpu'))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}